{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1676914614064,"user":{"displayName":"Sanmoy Bandyopadhyay","userId":"08293058629048558005"},"user_tz":-330},"id":"yRNnZ1ILlotz"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import re\n","import os\n","from urllib.parse import urljoin"]},{"cell_type":"markdown","metadata":{"id":"g9IHEMT_l0eC"},"source":["**BeautifulSoup** object is provided by Beautiful Soup which is a web scraping framework for Python. Web scraping is the process of extracting data from the website using automated tools to make the process faster. The BeautifulSoup object represents the parsed document as a whole. For most purposes, you can treat it as a Tag object.\n","\n","**Requests** library is an integral part of Python for making HTTP requests to a specified URL. Whether it be REST APIs or Web Scraping, requests must be learned for proceeding further with these technologies. When one makes a request to a URI, it returns a response. Python requests provide inbuilt functionalities for managing both the request and response."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1676914614064,"user":{"displayName":"Sanmoy Bandyopadhyay","userId":"08293058629048558005"},"user_tz":-330},"id":"OoDrdBa0l58P"},"outputs":[],"source":["# URL from which files to be downloaded\n","url = \"https://sdo.gsfc.nasa.gov/assets/img/browse/2017/04/01/\""]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1676914614065,"user":{"displayName":"Sanmoy Bandyopadhyay","userId":"08293058629048558005"},"user_tz":-330},"id":"axSRzX1hruYO"},"outputs":[],"source":["#If there is no such folder, the script will create one automatically\n","folder_location = r'/home/dev/Sun/Aditya_L1/day2/VSO_Data/Manual_Simplepy'\n","if not os.path.exists(folder_location):os.mkdir(folder_location)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":863,"status":"ok","timestamp":1676914614921,"user":{"displayName":"Sanmoy Bandyopadhyay","userId":"08293058629048558005"},"user_tz":-330},"id":"piv0muM6mVOv","outputId":"14903da8-a210-4f3b-c757-b35e70b480a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["<Response [403]>\n"]}],"source":["# Requests URL and get response object\n","response = requests.get(url)\n","print(response)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1676914614922,"user":{"displayName":"Sanmoy Bandyopadhyay","userId":"08293058629048558005"},"user_tz":-330},"id":"20N60jzLmcyY"},"outputs":[],"source":["# Parse text obtained\n","soup = BeautifulSoup(response.text, 'html.parser')"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1676914614922,"user":{"displayName":"Sanmoy Bandyopadhyay","userId":"08293058629048558005"},"user_tz":-330},"id":"9MGWCSq9t8S9","outputId":"2478d836-0960-43c9-8fac-5cbeeeeac0ce"},"outputs":[{"data":{"text/plain":["<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\n","\n","<html><head>\n","<title>403 Forbidden</title>\n","</head><body>\n","<h1>Forbidden</h1>\n","<p>You don't have permission to access /archive/soho/private/data/processed/lasco/level_05/120102/\n","on this server.<br/>\n","</p>\n","</body></html>"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["soup"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1676914614922,"user":{"displayName":"Sanmoy Bandyopadhyay","userId":"08293058629048558005"},"user_tz":-330},"id":"D1z1zXrFmiKi"},"outputs":[],"source":["for link in soup.select(\"a[href$='.fts']\"):\n","  #Name the fts files using the last portion of each link which are unique in this case\n","  filename = os.path.join(folder_location,link['href'].split('/')[-1])\n","  with open(filename, 'wb') as f:\n","    f.write(requests.get(urljoin(url,link['href'])).content)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPmW+0x4xs7k02Nl5pqsZjH","mount_file_id":"1lPVBkddk3mNv-H9341DvyZD5p7s1ZWQD","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
